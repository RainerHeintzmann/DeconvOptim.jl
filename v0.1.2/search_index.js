var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"See here for a list of references","category":"page"},{"location":"references/","page":"References","title":"References","text":"","category":"page"},{"location":"function_references/loss/#Loss-Functions","page":"Loss Functions","title":"Loss Functions","text":"","category":"section"},{"location":"function_references/loss/","page":"Loss Functions","title":"Loss Functions","text":"Poisson\npoisson_aux\nGauss\ngauss_aux\nScaledGauss\nscaled_gauss_aux","category":"page"},{"location":"function_references/loss/#DeconvOptim.Poisson","page":"Loss Functions","title":"DeconvOptim.Poisson","text":"Poisson()\n\nReturns a function to calculate Poisson loss Check the help of poisson_aux.\n\n\n\n\n\n","category":"function"},{"location":"function_references/loss/#DeconvOptim.poisson_aux","page":"Loss Functions","title":"DeconvOptim.poisson_aux","text":"poisson_aux(μ, meas)\n\nCalculates the Poisson loss for μ and meas. μ can be of larger size than meas. In that case we extract a centered region from μ of the same size as meas.\n\n\n\n\n\n","category":"function"},{"location":"function_references/loss/#DeconvOptim.Gauss","page":"Loss Functions","title":"DeconvOptim.Gauss","text":"Gauss()\n\nReturns a function to calculate Gauss loss. Check the help of gauss_aux.\n\n\n\n\n\n","category":"function"},{"location":"function_references/loss/#DeconvOptim.gauss_aux","page":"Loss Functions","title":"DeconvOptim.gauss_aux","text":"gauss_aux(μ, meas)\n\nCalculates the Gauss loss for μ and meas. μ can be of larger size than meas. In that case we extract a centered region from μ of the same size as meas.\n\n\n\n\n\n","category":"function"},{"location":"function_references/loss/#DeconvOptim.ScaledGauss","page":"Loss Functions","title":"DeconvOptim.ScaledGauss","text":"ScaledGauss()\n\nReturns a function to calculate scaled Gauss loss. Check the help of scaled_gauss_aux.\n\n\n\n\n\n","category":"function"},{"location":"function_references/loss/#DeconvOptim.scaled_gauss_aux","page":"Loss Functions","title":"DeconvOptim.scaled_gauss_aux","text":"scaled_gauss_aux(μ, meas)\n\nCalculates the scaled Gauss loss for μ and meas. μ can be of larger size than meas. In that case we extract a centered region from μ of the same size as meas.\n\n\n\n\n\n","category":"function"},{"location":"function_references/regularizer/#Regularizers","page":"Regularizers","title":"Regularizers","text":"","category":"section"},{"location":"function_references/regularizer/","page":"Regularizers","title":"Regularizers","text":"TV\nTikhonov\nGR","category":"page"},{"location":"function_references/regularizer/#DeconvOptim.TV","page":"Regularizers","title":"DeconvOptim.TV","text":"TV(; <keyword arguments>)\n\nThis function returns a function to calculate the Total Variation regularizer of a n-dimensional array. \n\nArguments\n\nnum_dims=2: \nsum_dims=[1, 2]: A array containing the dimensions we want to sum over\nweights=nothing: A array containing weights to weight the contribution of    different dimensions. If weights=nothing all dimensions are weighted equally.\nstep=1: A integer indicating the step width for the array indexing\nmode=\"central\": Either \"central\" or \"forward\" accounting for different   modes of the spatial gradient. Default is \"central\".\n\nExamples\n\nTo create a regularizer for a 3D dataset where the third dimension has different contribution. For the derivative we use forward mode.\n\njulia> reg = TV(num_dims=2, sum_dims=[1, 2], weights=[1, 1], mode=\"forward\");\n\njulia> reg([1 2 3; 4 5 6; 7 8 9])\n12.649111f0\n\n\n\n\n\n","category":"function"},{"location":"function_references/regularizer/#DeconvOptim.Tikhonov","page":"Regularizers","title":"DeconvOptim.Tikhonov","text":"Tikhonov(; <keyword arguments>)\n\nThis function returns a function to calculate the Tikhonov regularizer of a n-dimensional array. \n\nArguments\n\nnum_dims=2: \nsum_dims=[1, 2]: A array containing the dimensions we want to sum over\nweights=nothing: A array containing weights to weight the contribution of    different dimensions. If weights=nothing all dimensions are weighted equally.\nstep=1: A integer indicating the step width for the array indexing\nmode=\"laplace\": Either \"laplace\", \"spatial_grad_square\", \"identity\" accounting for different   modes of the Tikhonov regularizer. Default is \"laplace\".\n\nExamples\n\nTo create a regularizer for a 3D dataset where the third dimension has different contribution.\n\njulia> reg = Tikhonov(num_dims=2, sum_dims=[1, 2], weights=[1, 1], mode=\"identity\");\n\njulia> reg([1 2 3; 4 5 6; 7 8 9])\n285\n\n\n\n\n\n","category":"function"},{"location":"function_references/regularizer/#DeconvOptim.GR","page":"Regularizers","title":"DeconvOptim.GR","text":"GR(; <keyword arguments>)\n\nThis function returns a function to calculate the Good's roughness regularizer of a n-dimensional array. \n\nArguments\n\nnum_dims=2: Dimension of the array that should be regularized \nsum_dims=[1, 2]: A array containing the dimensions we want to sum over\nweights=nothing: A array containing weights to weight the contribution of    different dimensions. If weights=nothing all dimensions are weighted equally.\nstep=1: A integer indicating the step width for the array indexing\nmode=\"forward\": Either \"central\" or \"forward\" accounting for different   modes of the spatial gradient. Default is \"central\".\n\nExamples\n\nTo create a regularizer for a 3D dataset where the third dimension has different contribution. For the derivative we use forward mode.\n\njulia> reg = GR(num_dims=2, sum_dims=[1, 2], weights=[1, 1], mode=\"forward\");\n\njulia> reg([1 2 3; 4 5 6; 7 8 9])\n-26.36561871738898\n\n\n\n\n\n","category":"function"},{"location":"background/loss_functions/#Loss-functions","page":"Loss functions","title":"Loss functions","text":"","category":"section"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"Loss functions are generally introduced in mathematical optimization theory. The purpose is to map a certain optimization problem onto a real number. By minimizing this real number, one hopes that the obtained parameters provide a useful result for the problem.  One common loss function (especially in deep learning) is simply the L^2 norm between measurement and prediction.","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"So far we provide three adapted loss functions with our package. However, it is relatively easy to incorporate custom defined loss functions or import them from packages like Flux.ml. The interface from Flux.ml is the same as for our loss functions.","category":"page"},{"location":"background/loss_functions/#Poisson-Loss","page":"Loss functions","title":"Poisson Loss","text":"","category":"section"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"As mentioned in Noise Model, Poisson shot noise is usually the dominant source of noise. Therefore one achieves good results by choosing a loss function which considers both the difference between measurement and reconstruction but also the noise process. See Peter J. Verveer , Thomas M. Jovin  (1998) and Jerome Mertz  (2019) for more details on that. As key idea we interpret the measurement as a stochastic process. Our aim is to find a deconvolved image which describes as accurate as possible the measured image. Mathematically the probability for a certain measurement Y is","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"p(Y(r)mu(r)) = prod_r fracmu(r)^Y(r)Gamma(Y(r) + 1) exp(- mu(r))","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"where Y is the measurement, mu is the expected measurement (ideal measurement without noise) and Gamma is the generalized factorial function. In the deconvolution process we get Y as input and want to find the ideal specimen S which results in a measurement mu(r) = (S * textPSF)(r)). Since we want to find the best reconstruction, we want to find a mu(r) so that p(Y(r)  mu(r)) gets as large as possible. Because that means that we find the specimen which describes the measurement with the highest probability. Instead of maximizing p(Y(r)  mu(r)) a common trick is to minimize - log(p(Y(r)mu(r))).  Mathematically, the optimization of both functions provides same results but the latter is numerically more stable.","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"undersetS(r)arg min (- log(p(Y(r)mu(r)))) = undersetS(r)arg min sum_r (mu(r) + log(Gamma(Y(r) + 1)) - Y(r) log(mu(r))","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"which is equivalent to","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"undersetS(r)arg min L = undersetS(r)arg min sum_r (mu(r)  - Y(r) log(mu(r))","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"since the second term only depends on Y(r) but not on mu(r). The gradient of L with respect to mu(r) is simply","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"nabla L = 1 - fracY(r)mu(r)","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"The function L and the gradient nabla L are needed for any gradient descent optimization algorithm. The numerical evaluation of the Poisson loss can lead to issues. Since mu(r)=0 can happen for a measurement with zero intensity background. However, the loss is not defined for mu leq 0. In our source code we set all intensity values below a certain threshold epsilon to epsilon itself. This prevents the evaluation of the logarithm at undefined values.","category":"page"},{"location":"background/loss_functions/#Scaled-Gaussian-Loss","page":"Loss functions","title":"Scaled Gaussian Loss","text":"","category":"section"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"It is well known that the Poisson density function behaves similar as a Gaussian density function for mugg 1. This approximation is almost for all use cases in microscopy valid since regions of interest in an image usually consists of multiple photons and not to a single measured photon. Mathematically the Poisson probability can be approximately (using Stirling's formula in the derivation) expressed as:","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"p(Y(r)mu(r)) approx prod_r fracexp left(-frac(x-mu(r) )^22 mu(r) right)sqrt2 pi  mu(r) ","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"Applying the negative logarithm we get for the loss function:","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"undersetS(r)arg min L = undersetS(r)arg min sum_r frac12 log(mu(r)) + frac(Y(r)-mu(r))^22 mu(r)","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"The gradient is given by:","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"nabla L = fracmu(r) + mu(r)^2 - Y(r)^22 mu^2","category":"page"},{"location":"background/loss_functions/#Gaussian-Loss","page":"Loss functions","title":"Gaussian Loss","text":"","category":"section"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"A very common loss in optimization (and Deep Learning) is a simple Gaussian loss. However, this loss is not recommended for low intensity microscopy since it doesn't consider Poisson noise. However, still combined with suitable regularizer reasonable results can be achieved. The probability is defined as ","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"p(Y(r)mu(r)) = prod_r frac1sqrt2 pi sigma^2 expleft(- frac(Y(r) - mu(r))^22 sigma ^2 right)","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"where sigma is the standard deviation of the Gaussian.","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"Applying the negative logarithm we can simplify the loss to be minimized:","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"undersetS(r)arg min L = undersetS(r)arg min sum_r (Y(r) - mu(r))^2","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"Since we are looking for mu(r) minimizing this expression, sigma is just a constant offset being irrelevant for the solution. This expression is also called L2 loss.","category":"page"},{"location":"function_references/mapping/#Mapping-Functions","page":"Mapping Functions","title":"Mapping Functions","text":"","category":"section"},{"location":"function_references/mapping/","page":"Mapping Functions","title":"Mapping Functions","text":"Non_negative","category":"page"},{"location":"function_references/mapping/#DeconvOptim.Non_negative","page":"Mapping Functions","title":"DeconvOptim.Non_negative","text":"Non_negative()\n\nReturns a function and a inverse function inverse function to map numbers to non-negative numbers. We use a parabola.\n\nExamples\n\njulia> p, p_inv = Non_negative()\n(DeconvOptim.var\"#5#7\"(), DeconvOptim.var\"#6#8\"())\n\njulia> x = [-1, 2, -3]\n3-element Array{Int64,1}:\n -1\n  2\n -3\n\njulia> p(x)\n3-element Array{Int64,1}:\n 1\n 4\n 9\n\njulia> p_inv(p(x))\n3-element Array{Float64,1}:\n 1.0\n 2.0\n 3.0\n\n\n\n\n\n","category":"function"},{"location":"workflow/changing_regularizers/#Changing-Regularizers:-2D-Example","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"","category":"section"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"In this section we show how to change the regularizer and what are the different effects of it. The arguments of deconvolution we consider here are regularizer and lambda. regularizer specifies which regularizer is used. lambda specifies how strong the regularizer is weighted. The larger lambda the more you see the typical styles introduced by the regularizers.","category":"page"},{"location":"workflow/changing_regularizers/#Initializing","page":"Changing Regularizers: 2D Example","title":"Initializing","text":"","category":"section"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"This example is also hosted in a notebook on GitHub.","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"Load the required modules for these examples:","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"using DeconvOptim, TestImages, Images, FFTW, Noise, ImageView\n\n# custom image views\nimshow_m(args...) = imshow(cat(args..., dims=3))\nh_view(args...) = begin\n    img = cat(args..., dims=2)\n    img ./= maximum(img)\n    colorview(Gray, img)\nend","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"As the next step we can prepare a noisy, blurred image.","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"# load test images\nimg = convert(Array{Float32}, channelview(testimage(\"resolution_test_512\")))\n\npsf = generate_psf(size(img), 30)\n\n# create a blurred, noisy version of that image\nimg_b = conv_psf(img, psf, [1, 2])\nimg_n = poisson(img_b, 300);\n\nh_view(img, img_b, img_n)","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"(Image: )","category":"page"},{"location":"workflow/changing_regularizers/#Let's-test-Good's-roughness-(GR)","page":"Changing Regularizers: 2D Example","title":"Let's test Good's roughness (GR)","text":"","category":"section"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"In this part we can look at the results produced with a GR regularizer. After inspecting the results, it becomes clear, that the benefit of 100 iterations is not really visible. In most cases approx 15 iterations produce good results. By executing GR() we in fact create a function which takes a array and returns a single value. ","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"@time resGR100, optim_res = deconvolution(img_n, psf, regularizer=GR(), iterations=100)\n@show optim_res\n\n@time resGR15, optim_res = deconvolution(img_n, psf, regularizer=GR(), iterations=15)\n@show optim_res\n\n@time resGR15_2, optim_res = deconvolution(img_n, psf, λ=0.05, regularizer=GR(), iterations=15)\n@show optim_res\n\nh_view(img_n, resGR100, resGR15, resGR15_2)","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"(Image: )","category":"page"},{"location":"workflow/changing_regularizers/#Let's-test-Total-Variation-(TV)","page":"Changing Regularizers: 2D Example","title":"Let's test Total Variation (TV)","text":"","category":"section"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"TV produces characteristic staircase artifacts. However, the results it produces are usually noise free and clear.","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"@time resTV50, optim_res = deconvolution(img_n, psf, regularizer=TV(), iterations=50)\n@show optim_res\n\n@time resTV15, optim_res = deconvolution(img_n, psf, regularizer=TV(), iterations=15)\n@show optim_res\n\n@time resTV15_2, optim_res = deconvolution(img_n, psf, λ=0.005, regularizer=TV(), iterations=15)\n@show optim_res\n\nh_view(img_n, resTV50, resTV15, resTV15_2)","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"(Image: )","category":"page"},{"location":"workflow/changing_regularizers/#Let's-test-Tikhonov","page":"Changing Regularizers: 2D Example","title":"Let's test Tikhonov","text":"","category":"section"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"Tikhonov is not defined as precisely as the other two regularizers. Therefore we offer three different modes which differ quite a lot from each other. However, the results look all very similar","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"@time resTik1, optim_res = deconvolution(img_n, psf, λ=0.001, regularizer=Tikhonov(), iterations=15)\n@show optim_res\n\n\n@time resTik2, optim_res = deconvolution(img_n, psf, λ=0.0001, \n                    regularizer=Tikhonov(mode=\"spatial_grad_square\"), iterations=15)\n@show optim_res\n\n@time resTik3, optim_res = deconvolution(img_n, psf, λ=0.0001, \n    regularizer=Tikhonov(mode=\"identity\"), iterations=15)\n@show optim_res\n\nh_view(img_n, resTik1, resTik2, resTik3)","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"(Image: )","category":"page"},{"location":"workflow/changing_regularizers/#Let's-test-without-regularizers","page":"Changing Regularizers: 2D Example","title":"Let's test without regularizers","text":"","category":"section"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"Usually optimizing without a regularizer is does not produce good results. The reason is, that the deconvolution tries to enhance high frequencies more and more with increasing iteration number.  However, high frequencies have low contrast and therefore the algorithm mostly enhances noise content (which is present in all frequency regions).","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"@jldoctest\n@time res100, optim_res = deconvolution(img_n, psf, regularizer=nothing, iterations=50)\n@show optim_res\n\n@time res15, optim_res = deconvolution(img_n, psf, regularizer=nothing, iterations=15)\n@show optim_res\n\nh_view(img_n, 0.7 .* res100, res15)","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"(Image: )","category":"page"},{"location":"background/physical_background/#Physical-Background","page":"Physical Background","title":"Physical Background","text":"","category":"section"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"We want to provide some physical background to the process of (de)convolution in optics. Optical systems like brightfield microscopes can only collect a certain amount of light emitted by a specimen. This effect (diffraction) leads to a blurred image of that specimen. Mathematically the lens has a certain frequency support. Within that frequency range, transmission of light is supported. Information (light) outside of this frequency support (equivalent to high frequency information) is lost. In the following picture we can see several curves in the frequency domain.  The orange line is a artificial object with a constant frequency spectrum (delta peak in real space). If such a delta peak is transferred through an optical lens, in real space the object is convolved with the point spread function (PSF).  In frequency space such a convolution is a multiplication of the OTF (OTF is the Fourier transform of the PSF) and the frequency spectrum of the object. The green dotted curve is the captured image after transmission through the system. Additionally some noise was introduced which can be recognized through some bumps outside of the OTF support. (Image: Frequency spectrum)","category":"page"},{"location":"background/physical_background/#Forward-Model","page":"Physical Background","title":"Forward Model","text":"","category":"section"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"Mathematically an ideal imaging process of specimen emitting incoherent light by a lens (or any optical system in general) can be described as:","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"Y(r) = (S * textPSF)(r)","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"where * being a convolution operation, r being the position, S being the sample and textPSF being the point spread function of the system. One can also introduce a background term b independent of the position, which models a constant signal offset of the imaging sensor:","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"Y(r) = (S * textPSF)(r) + b","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"In frequency space (Fourier transforming the above equation) the equation with b=0 is:","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"tilde Y(k) = (tilde S * tildetextPSF)(k)","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"where k is the spatial frequency. From that equation it is clear why the green and blue line in the plot look very similar. The reason is, that the orange line is constant and we basically multiply the OTF with the orange line. ","category":"page"},{"location":"background/physical_background/#Noise-Model","page":"Physical Background","title":"Noise Model","text":"","category":"section"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"However, the physical description (forward model) should also contain a noise term to reflect the measurement process in reality more accurately. ","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"Y(r) = (S * textPSF)(r) + N(r) = mu(r) + N(r)","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"where N being a noise term. In fluorescence microscopy the dominant noise is usually Poisson shot noise (see Jerome Mertz  (2019)). The origin of that noise is the quantum nature of photons. Since the measurement process spans over a time T only a discrete number of photons is detected (in real experiment the amount of photons per pixel is usually in the order of 10^1 - 10^3). Note that this noise is not introduced by the sensor and is just a effect due to quantum nature of light.  We can interprete every sensor pixel as a discrete random variable X. The expected value of that pixel would be mu(r) (true specimen convolved with the textPSF). Due to noise, the systems measures randomly a signal for X according to the Poisson distribution:","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"f(y mu) = fracmu^y exp(-mu)Gamma(y + 1)","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"where f is the probability density distribution, y the measured value of the sensor, mu the expected value and Gamma the generalized factorial function.","category":"page"},{"location":"workflow/3D_dataset/#D-Dataset","page":"3D Dataset","title":"3D Dataset","text":"","category":"section"},{"location":"workflow/3D_dataset/","page":"3D Dataset","title":"3D Dataset","text":"We can also deconvolve a 3D dataset with a 3D PSF. The workflow, especially for the regularizers, must be adapted slightly for 3D.","category":"page"},{"location":"workflow/3D_dataset/#Code-Example","page":"3D Dataset","title":"Code Example","text":"","category":"section"},{"location":"workflow/3D_dataset/","page":"3D Dataset","title":"3D Dataset","text":"This example is also hosted in a notebook on GitHub.","category":"page"},{"location":"workflow/3D_dataset/","page":"3D Dataset","title":"3D Dataset","text":"First, load the 3D PSF and image.","category":"page"},{"location":"workflow/3D_dataset/","page":"3D Dataset","title":"3D Dataset","text":"using Revise, DeconvOptim, TestImages, Images, FFTW, Noise, ImageView\nimg = convert(Array{Float32}, channelview(load(\"obj.tif\")))\npsf = ifftshift(convert(Array{Float32}, channelview(load(\"psf.tif\"))))\npsf ./= sum(psf)\n# create a blurred, noisy version of that image\nimg_b = conv_psf(img, psf, [1, 2, 3])\nimg_n = poisson(img_b, 300);","category":"page"},{"location":"workflow/3D_dataset/","page":"3D Dataset","title":"3D Dataset","text":"As the next step we need to create the regularizers. With num_dims we define how many dimensions our reconstruction image has.  With sum_dims we specify which dimensions of those should be included in the regularizing process.","category":"page"},{"location":"workflow/3D_dataset/","page":"3D Dataset","title":"3D Dataset","text":"reg1 = TV(num_dims=3, sum_dims=[1, 2, 3])\nreg2 = Tikhonov(num_dims=3, sum_dims=[1, 2, 3], mode=\"identity\")","category":"page"},{"location":"workflow/3D_dataset/","page":"3D Dataset","title":"3D Dataset","text":"We can then invoke the deconvolution. For Tikhonov using identity mode a smaller lambda produces better results. In the first reconstruction we also specified the padding. This parameters adds some spacing around the reconstruction image to prevent wrap around effects of the FFT based deconvolution. However, since we don't have bright objects at the boundary of the image we don't see an impact of that parameter.","category":"page"},{"location":"workflow/3D_dataset/","page":"3D Dataset","title":"3D Dataset","text":"@time res, ores = deconvolution(img, psf, regularizer=reg1, loss=Poisson(),\n                          λ=0.05, padding=0.2, iterations=10);\n@time res2, ores = deconvolution(img, psf, regularizer=reg2, loss=Poisson(),\n                          λ=0.001, padding=0.0, iterations=10);","category":"page"},{"location":"workflow/3D_dataset/","page":"3D Dataset","title":"3D Dataset","text":"Finally we can inspect the results:","category":"page"},{"location":"workflow/3D_dataset/","page":"3D Dataset","title":"3D Dataset","text":"img_comb1 = [img[:, : ,32] res2[:, :, 32] res[:, :, 32] img_n[:, :, 32]]\nimg_comb2 = [img[:, : ,38] res2[:, :, 38] res[:, :, 38] img_n[:, :, 38]]\n\nimg_comb = cat(img_comb1, img_comb2, dims=1)\nimg_comb ./= maximum(img_comb)\n\nimshow([img[:, :, 20:end] res2[:, :, 20:end] res[:, :, 20:end] img_n[:, :, 20:end]])\ncolorview(Gray, img_comb)","category":"page"},{"location":"workflow/3D_dataset/","page":"3D Dataset","title":"3D Dataset","text":"(Image: )","category":"page"},{"location":"function_references/deconvolution/#Deconvolution","page":"Deconvolution","title":"Deconvolution","text":"","category":"section"},{"location":"function_references/deconvolution/","page":"Deconvolution","title":"Deconvolution","text":"deconvolution","category":"page"},{"location":"function_references/deconvolution/#DeconvOptim.deconvolution","page":"Deconvolution","title":"DeconvOptim.deconvolution","text":"deconvolution(measured, psf; <keyword arguments>)\n\nComputes the deconvolution of measured and psf. Return parameter is a tuple with two elements. The first entry is the deconvolved image. The second return parameter  is the output of the optimization of Optim.jl\n\nMultiple keyword arguments can be specified for different loss functions, regularizers and mappings.\n\nArguments\n\nloss=Poisson(): the loss function taking a vector the same shape as measured. \nregularizer=GR(): A regularizer function, same form as loss.\nλ=0.05: A float indicating the total weighting of the regularizer with    respect to the global loss function\nbackground=0: A float indicating a background intensity level.\nmapping=Non_negative(): Applies a mapping of the optimizer weight. Default is a              parabola which achieves a non-negativity constraint.\niterations=20: Specifies a number of iterations after the optimization.   definitely should stop.\nplan_fft=true: Boolean whether plan_fft is used. Gives a slight speed improvement.\npadding=0: an float indicating the amount (fraction of the size in that dimension)        of padded regions around the reconstruction. Prevents wrap around effects of the FFT.       A array with size(arr)=(400, 400) with padding=0.05 would result in reconstruction size of        (440, 440). However, we only return the reconstruction cropped to the original size.       padding=0 disables any padding.\noptim_options=nothing: Can be a options file required by Optim.jl. Will overwrite iterations.\noptim_optimizer=LBFGS(): The chosen Optim.jl optimizer. \n\nExample\n\njulia> using DeconvOptim, TestImages, Images, FFTW, Noise, ImageView;\n\njulia> img = channelview(testimage(\"resolution_test_512\"));\n\njulia> psf = generate_psf(size(img), 30);\n\njulia> img_b = conv_psf(img, psf);\n\njulia> img_n = poisson(img_b, 300);\n\njulia> @time res, o = deconvolution(img_n, psf);\n\njulia> colorview(Gray, [img img_n res]) |> imshow\n[...]\n\n\n\n\n\n","category":"function"},{"location":"workflow/changing_loss/#Changing-Loss-Function:-2D-Example","page":"Changing Loss Function: 2D Example","title":"Changing Loss Function: 2D Example","text":"","category":"section"},{"location":"workflow/changing_loss/","page":"Changing Loss Function: 2D Example","title":"Changing Loss Function: 2D Example","text":"We can also change the loss function. However, the loss is the most important part guaranteeing good results. Therefore choosing different loss functions than the  provided ones, will most likely lead to worse results. We now compare all implemented loss functions of DeconvOptim.jl. However, we could also include loss functions of Flux.jl since they have the same interface as our loss functions.","category":"page"},{"location":"workflow/changing_loss/","page":"Changing Loss Function: 2D Example","title":"Changing Loss Function: 2D Example","text":"Poisson() will most likely produce the best results in presence of Poisson Noise. For Gaussian Noise, Gauss() is a suitable option. ScaledGaussian() is an mathematical approximation of Poisson(). At the moment ScaledGaussian() is not recommended because of artifacts in certain images.","category":"page"},{"location":"workflow/changing_loss/#Code-Example","page":"Changing Loss Function: 2D Example","title":"Code Example","text":"","category":"section"},{"location":"workflow/changing_loss/","page":"Changing Loss Function: 2D Example","title":"Changing Loss Function: 2D Example","text":"This example is also hosted in a notebook on GitHub.","category":"page"},{"location":"workflow/changing_loss/","page":"Changing Loss Function: 2D Example","title":"Changing Loss Function: 2D Example","text":"using Revise, DeconvOptim, TestImages, Images, FFTW, Noise, ImageView\n\n# custom image views\nimshow_m(args...) = imshow(cat(args..., dims=3))\nh_view(args...) = begin\n    img = cat(args..., dims=2)\n    img ./= maximum(img)\n    colorview(Gray, img)\nend\n\n# load test images\nimg = convert(Array{Float32}, channelview(testimage(\"resolution_test_512\")))\n\npsf = generate_psf(size(img), 30)\n\n# create a blurred, noisy version of that image\nimg_b = conv_psf(img, psf, [1, 2])\nimg_n = poisson(img_b, 300);\n\n@time resP, optim_res = deconvolution(img_n, psf, loss=Poisson(), iterations=10)\n@show optim_res\n\n@time resG, optim_res = deconvolution(img_n, psf, loss=Gauss(), iterations=10)\n@show optim_res\n\n@time resSG, optim_res = deconvolution(img_n, psf, loss=ScaledGauss(), iterations=10)\n@show optim_res\n\nh_view(resP, resG, resSG)","category":"page"},{"location":"workflow/changing_loss/","page":"Changing Loss Function: 2D Example","title":"Changing Loss Function: 2D Example","text":"The left image is Poisson(), in the middle Gauss(). The right image is ScaledGauss(). (Image: )","category":"page"},{"location":"background/mathematical_optimization/#Mathematical-Optimization","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"","category":"section"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"Deconvolution was already described as an optimization problem in the 1970s by L.~B. { Lucy }  (1974), William Hadley Richardson  (1972). Since then, many variants and different kinds of deconvolution algorithms were presented, but mainly based on the concept of Lucy-Richardson. We try to formulate convolution as an inverse physical problem and solve it using a convex optimization loss function so that we can use fast optimizers to find the optimum. The variables we want to optimize for, are the pixels of the reconstruction S(r). Therefore our reconstruction problem consists of several thousands to billion variables. Mathematically the optimization can be written as:","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"undersetS(r)arg min L(textFwd(S(r))) + textReg(S(r))","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"where textFwd represents the forward model (in our case convolution of S(r) with the textPSF), S(r) is ideal reconstruction, L the loss function and textReg is a regularizer. The regularizer  puts in some prior information about the structure of the object.  See the following sections for more details about each part.","category":"page"},{"location":"background/mathematical_optimization/#Map-Functions","page":"Mathematical Optimization","title":"Map Functions","text":"","category":"section"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"In some cases we want to restrict the optimizer to solutions with S(r) geq 0. Usually one uses boxed optimizer or penalties to prevent negativity. However, in some cases, a S(r)  0 can lead to issues during the optimization process. For that purpose we can introduce a mapping function. Instead of optimizing for S(r) we can optimize for some hat S(r) where M is the mapping function connection ","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"S(r)= M(hat S(r))","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"A simple mapping function leading to S(r) geq 0 is ","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"M(hat S(r)) = hat S(r)^2","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"The optimization problem is then given by","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"undersethat S(r)arg min L(textFwd(M(hat S(r)))) + textReg(M(hat S(r)))","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"After the optimization we need to apply M on hat S to get the reconstructed sample ","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"S(r) = M(hat S(r))","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"One could also choose different functions M to obtain reconstruction in certain intensity intervals.","category":"page"},{"location":"background/regularizer/#Regularizer","page":"Regularizer","title":"Regularizer","text":"","category":"section"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"Regularizer are commonly used in inverse problems and especially in deconvolution to obtain solutions which are optimal with respect to some prior.  So far we have included three common regularizer. The regularizer take the current reconstruction S(r) as argument and return a scalar value. This value should be also minimized and is also added to the loss function. Each regularizer produces some characteristic image styles.","category":"page"},{"location":"background/regularizer/#Good's-Roughness-(GR)","page":"Regularizer","title":"Good's Roughness (GR)","text":"","category":"section"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"The Good's roughness definition was taken from I. J. Good , R. A. Gaskins  (1971) and Peter J. Verveer , Thomas M. Jovin  (1998). For Good's roughness several identical expressions can be derived. We implemented the following one:","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"textReg(S(r)) = sum_r sqrtS(r) (Delta_N sqrtS)(r)","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"where N is the dimension of S(r). sqrt S is applied elementwise. Delta_n sqrtS(r) is the n-dimensional discrete Laplace operator. As 2D example where r = (xy):","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"(Delta_n sqrtS)(r) = fracsqrtS(x + s_x y) + sqrtS(x - s_x y) + sqrtS(x y+s_y) + sqrtS(x y-s_y) - 4 cdot sqrtS(x y)s_x cdot s_y","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"where s_x and s_y are the stencil width in the respective dimension. The Laplace operator can be straightforwardly generalized to n dimensions. ","category":"page"},{"location":"background/regularizer/#Total-Variation-(TV)","page":"Regularizer","title":"Total Variation (TV)","text":"","category":"section"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"As the name suggests, Total variation tries to penalize variation in the image intensity. Therefore it sums up the gradient strength at each point of the image. In 2D this is:","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"textReg(S(r)) = sum_r  (nabla S)(r)","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"Since we look at the magnitude of the gradient strength, this regularizer is anisotropic.","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"In 2D this is:","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"textReg(S(r)) = sum_xy sqrtS(x + 1 y) - S(x y)^2 + S(x y + 1) - S(x y)^2","category":"page"},{"location":"background/regularizer/#Tikhonov-Regularization","page":"Regularizer","title":"Tikhonov Regularization","text":"","category":"section"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"The Tikhonov regularizer is not as specific defined as Good's Roughness or Total Variation. In general Tikhonov regularization is defined by:","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"textReg(S(r)) =  (Gamma S)(r) _2^2","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"where Gamma is an operator which can be chosen freely. Common options are the identity operator which penalizes therefore just high intensity values. Another option would be the spatial gradient which would result in a similar operator to TV. And the last option we implemented is the spatial Laplace.","category":"page"},{"location":"function_references/utils/#Util-functions","page":"Util functions","title":"Util functions","text":"","category":"section"},{"location":"function_references/utils/#Convolution-Functions","page":"Util functions","title":"Convolution Functions","text":"","category":"section"},{"location":"function_references/utils/","page":"Util functions","title":"Util functions","text":"conv_psf\nconv_otf\nconv_otf_r\nplan_conv_r","category":"page"},{"location":"function_references/utils/#DeconvOptim.conv_psf","page":"Util functions","title":"DeconvOptim.conv_psf","text":"conv_psf(obj, psf [, dims])\n\nConvolve obj with psf over dims dimensions. Based on FFT convolution.\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#DeconvOptim.conv_otf","page":"Util functions","title":"DeconvOptim.conv_otf","text":"conv_otf(obj, otf [, dims])\n\nPerforms a FFT-based convolution of an obj with an otf. otf = fft(psf). The 0 frequency of the otf must be located at position [1, 1, 1]. The obj can be of arbitrary dimension but ndims(psf) ≥ ndims(otf). The convolution happens over the dims array. Any further dimensions are broadcasted. Per default dims = [1, 2].\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#DeconvOptim.conv_otf_r","page":"Util functions","title":"DeconvOptim.conv_otf_r","text":"conv_otf_r(obj, otf [, dims])\n\nPerforms a FFT-based convolution of an obj with an otf. Same arguments as conv_otf but with obj being real and otf=rfft(psf).\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#DeconvOptim.plan_conv_r","page":"Util functions","title":"DeconvOptim.plan_conv_r","text":"plan_conv_r(psf [, dims])\n\nPre-plan an optimized convolution for array shaped like psf (based on pre-plan FFT)  along the given dimenions dims. dims = [1, 2] per default. The 0 frequency of psf must be located at [1, 1, 1]. We return first the otf (obtained by rfft(psf)). The second return is the convolution function conv. conv itself has two arguments. conv(obj, otf) where obj is the object and otf the otf.\n\nThis function achieves faster convolution than conv_psf(obj, psf).\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#Point-Spread-Function","page":"Util functions","title":"Point Spread Function","text":"","category":"section"},{"location":"function_references/utils/","page":"Util functions","title":"Util functions","text":"generate_psf","category":"page"},{"location":"function_references/utils/#DeconvOptim.generate_psf","page":"Util functions","title":"DeconvOptim.generate_psf","text":"generate_psf(psf_size, radius)\n\nGeneration of an approximate 2D PSF. psf_size is the output size of the PSF. The PSF will be centered around the point [1, 1], radius indicates the pupil diameter in pixel from which the PSF is generated.\n\nExamples\n\njulia> generate_psf([5, 5], 2)\n5×5 Array{Float64,2}:\n 0.36       0.104721    0.0152786    0.0152786    0.104721\n 0.104721   0.0304627   0.00444444   0.00444444   0.0304627\n 0.0152786  0.00444444  0.000648436  0.000648436  0.00444444\n 0.0152786  0.00444444  0.000648436  0.000648436  0.00444444\n 0.104721   0.0304627   0.00444444   0.00444444   0.0304627\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#Interpolation-and-downsampling","page":"Util functions","title":"Interpolation and downsampling","text":"","category":"section"},{"location":"function_references/utils/","page":"Util functions","title":"Util functions","text":"generate_downsample\nmy_interpolate","category":"page"},{"location":"function_references/utils/#DeconvOptim.generate_downsample","page":"Util functions","title":"DeconvOptim.generate_downsample","text":"generate_downsample(num_dim, downsample_dims, factor)\n\nGenerate a function (based on Tullio.jl) which can be used to downsample arrays. num_dim (Integer) are the dimensions of the array. downsample_dims is a list of which dimensions should be downsampled. factor is a downsampling factor. It needs to be an integer number.\n\nExamples\n\njulia> ds = generate_downsample(2, [1, 2], 2) \n[...]\njulia> ds([1 2; 3 4; 5 6; 7 8])\n2×1 Array{Float64,2}:\n 2.5\n 6.5\n\njulia> ds = generate_downsample(2, [1], 2)\n[...]\njulia> ds([1 2; 3 5; 5 6; 7 8])\n2×2 Array{Float64,2}:\n 2.0  3.5\n 6.0  7.0\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#DeconvOptim.my_interpolate","page":"Util functions","title":"DeconvOptim.my_interpolate","text":"my_interpolate(arr, size_n, [interp_type])\n\nInterpolates arr to the sizes provided in size_n. Therefore it holds ndims(arr) == length(size_n). interp_type specifies the interpolation type. See Interpolations.jl for all options\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#Center-Methods","page":"Util functions","title":"Center Methods","text":"","category":"section"},{"location":"function_references/utils/","page":"Util functions","title":"Util functions","text":"center_extract\ncenter_set!\ncenter_pos","category":"page"},{"location":"function_references/utils/#DeconvOptim.center_extract","page":"Util functions","title":"DeconvOptim.center_extract","text":"center_extract(arr, new_size)\n\nExtracts a center of an array.  new_size must be list of sizes indicating the output size of each dimension. Centered means that a center frequency stays at the center position. Works for even and uneven. If length(new_size) < length(size(arr)) the remaining dimensions are untouched and copied.\n\nExamples\n\njulia> center_extract([[1,2] [3, 4]], [1])\n1×2 Array{Int64,2}:\n 2  4\n\njulia> center_extract([[1,2] [3, 4]], [1, 1])\n1×1 Array{Int64,2}:\n4\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#DeconvOptim.center_set!","page":"Util functions","title":"DeconvOptim.center_set!","text":"center_set!(arr_large, arr_small)\n\nPuts the arr_small central into arr_large. The convention, where the center is, is the same as the definition as for FFT based centered. Function works both for even and uneven arrays.\n\nExamples\n\njulia> center_set!([1, 1, 1, 1, 1, 1], [5, 5, 5])\n6-element Array{Int64,1}:\n 1\n 1\n 5\n 5\n 5\n 1\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#DeconvOptim.center_pos","page":"Util functions","title":"DeconvOptim.center_pos","text":"center_pos(x)\n\nCalculate the position of the center frequency. Size of the array is x\n\nExamples\n\njulia> center_pos(3)\n2\njulia> center_pos(4)\n4\n\n\n\n\n\n","category":"function"},{"location":"workflow/basic_workflow/#Basic-Workflow","page":"Basic Workflow","title":"Basic Workflow","text":"","category":"section"},{"location":"workflow/basic_workflow/","page":"Basic Workflow","title":"Basic Workflow","text":"In this section we show the workflow for deconvolution of 2D and 3D images using different regularizers.  From these examples one can also understand the different effects of the regularizers.","category":"page"},{"location":"workflow/basic_workflow/","page":"Basic Workflow","title":"Basic Workflow","text":"The picture below shows the general principle of DeconvOptim.jl. Since we interprete deconvolution as an optimization we initialize the reconstruction variables rec.  rec is a array of pixels which are the variables we are optimizing for. Then we can apply some mapping eg. to reconstruct only pixels having non-negative intensity values. Afterwards we compose the total loss functions. It consists of a regularizing part (weighted with lambda) and a loss part. The latter one compares the current reconstruction with the measured image. Total loss adds both values to a single scalar value. Using Zygote.jl we calculate the gradient with respect to all pixel values of rec. Note, Zygote.jl calculates the gradient with a reverse mode. From performance point of view, that is necessary since the loss function is a mapping from many pixels to a single value (texttotal loss mathbbR^N mapsto mathbbR). We can plug this gradient and the loss function into Optim.jl. Optim.jl then minimizes this loss function. The different parts of the pipeline (mapping, forward, regularizer) can be exchanged and adapted to the users needs. In most cases changing the regularizer or the number of iterations is enough.","category":"page"},{"location":"workflow/basic_workflow/","page":"Basic Workflow","title":"Basic Workflow","text":"(Image: )","category":"page"},{"location":"workflow/basic_workflow/","page":"Basic Workflow","title":"Basic Workflow","text":"For all options, see the function references. Via the help of Julia (typing ] in the REPL) we can also access extensive help.","category":"page"},{"location":"#DeconvOptim.jl","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"","category":"section"},{"location":"","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"A framework for deconvolution of images convolved with a Point Spread Function (PSF).","category":"page"},{"location":"#Overview","page":"DeconvOptim.jl","title":"Overview","text":"","category":"section"},{"location":"","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"In optics, especially in microscopy, measurements are done with lenses. These lenses support only certain frequencies and weaken the contrast of high frequency content. Furthermore, in many cases Poisson or Gaussian noise is introduced by the  quantum nature of light (Poisson shot noise) or sensors (readout noise). DeconvOptim.jl is a Julia solution to deconvolution reducing the blur of lenses and denoising the image. Our framework relies on several other tools: The deconvolution problem is stated as a convex optimization problem via a loss function. Hence we make use of Optim.jl and especially fast solvers like L-BFGS. Since such solvers require gradients (of the loss function) we use automatic differentiation (AD) offered by Zygote.jl for that. Of course, one could derive the gradient by hand, however that's error-prone and for some regularizers hard to do by hand. Furthermore, fast AD of the regularizers is hard to achieve if the gradients are written with for loops. Fortunately Tullio.jl provides an extensive and fast framework to get expressions which can derived by the AD in acceptable speed.","category":"page"},{"location":"#Installation","page":"DeconvOptim.jl","title":"Installation","text":"","category":"section"},{"location":"","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"To get the latest stable release of DeconvOptim.jl type ] in the Julia REPL:","category":"page"},{"location":"","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"] add DeconvOptim","category":"page"},{"location":"#Quick-Example","page":"DeconvOptim.jl","title":"Quick Example","text":"","category":"section"},{"location":"","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"Below is a quick example how to deconvolve a image which is blurred with a Gaussian Kernel.","category":"page"},{"location":"","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"using DeconvOptim, TestImages, Images, FFTW, Noise\n\n# load test image\nimg = channelview(testimage(\"resolution_test_512\"))\n\n# generate simple Point Spread Function of aperture radius 30\npsf = generate_psf(size(img), 30)\n\n# create a blurred, noisy version of that image\nimg_b = conv_psf(img, psf)\nimg_n = poisson(img_b, 300)\n\n# deconvolve 2D with default options\n@time res, o = deconvolution(img_n, psf)\n\n# show final results next to original and blurred version\ncolorview(Gray, [img img_n res])","category":"page"},{"location":"","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"Left image is the sample. In the middle we display the the noisy and blurred version captured with an optical system. The right image is the deconvolved image with default options. (Image: )","category":"page"}]
}
