var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"See here for a list of references","category":"page"},{"location":"references/","page":"References","title":"References","text":"","category":"page"},{"location":"function_references/regularizer/#Regularizers","page":"Regularizers","title":"Regularizers","text":"","category":"section"},{"location":"function_references/regularizer/","page":"Regularizers","title":"Regularizers","text":"TV\nTikhonov\nGR","category":"page"},{"location":"function_references/regularizer/#DeconvOptim.TV","page":"Regularizers","title":"DeconvOptim.TV","text":"TV(; <keyword arguments>)\n\nThis function returns a function to calculate the Total Variation regularizer of a n-dimensional array. \n\nArguments\n\nnum_dim=2: \nsum_dims=[1, 2, 3]: A array containing the dimensions we want to sum over\nweights=[1, 1, 0.25]: A array containing weights to weight the contribution of    different dimensions\nstep=1: A integer indicating the step width for the array indexing\nmode=\"central\": Either \"central\" or \"forward\" accounting for different   modes of the spatial gradient. Default is \"central\".\n\nExamples\n\nTo create a regularizer for a 3D dataset where the third dimension has different contribution. For the derivative we use forward mode.\n\njulia> TV(num_dim=3, sum_dims=[1, 2, 3], weights=[1, 1, 0.25], mode=\"forward\")\n\n\n\n\n\n","category":"function"},{"location":"function_references/regularizer/#DeconvOptim.Tikhonov","page":"Regularizers","title":"DeconvOptim.Tikhonov","text":"Tikhonov(; <keyword arguments>)\n\nThis function returns a function to calculate the Tikhonov regularizer of a n-dimensional array. \n\nArguments\n\nnum_dim=2: \nsum_dims=[1, 2]: A array containing the dimensions we want to sum over\nweights=[1, 1]: A array containing weights to weight the contribution of    different dimensions\nstep=1: A integer indicating the step width for the array indexing\nmode=\"laplace\": Either \"laplace\", \"spatial_grad_square\", \"identity\" accounting for different   modes of the Tikhonov regularizer. Default is \"laplace\".\n\nExamples\n\nTo create a regularizer for a 3D dataset where the third dimension has different contribution.\n\njulia> Tikhonov(sum_dims=[1, 2, 3], weights=[1, 1, 0.25])\n\n\n\n\n\n","category":"function"},{"location":"function_references/regularizer/#DeconvOptim.GR","page":"Regularizers","title":"DeconvOptim.GR","text":"GR(; <keyword arguments>)\n\nThis function returns a function to calculate the Good's roughness regularizer of a n-dimensional array. \n\nArguments\n\nnum_dim=2: Dimension of the array that should be regularized \nsum_dims=[1, 2]: A array containing the dimensions we want to sum over\nweights=[1, 1]: A array containing weights to weight the contribution of    different dimensions\nstep=1: A integer indicating the step width for the array indexing\nmode=\"forward\": Either \"central\" or \"forward\" accounting for different   modes of the spatial gradient. Default is \"central\".\n\nExamples\n\nTo create a regularizer for a 3D dataset where the third dimension has different contribution. For the derivative we use forward mode.\n\njulia> GR(sum_dims=[1, 2, 3], weights=[1, 1, 0.25], mode=\"forward\")\n\n\n\n\n\n","category":"function"},{"location":"background/loss_functions/#Loss-functions","page":"Loss functions","title":"Loss functions","text":"","category":"section"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"Loss functions are generally introduced in mathematical optimization theory. The purpose is to map a certain optimization problem onto a real number. By minimising this real number, one hopes that the obtained parameters provide a useful result for the problem.  One common loss function (especially in deep learning) is simply the L^2 norm between measurement and prediction.","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"So far we provide two adapted loss functions with our package. However, it is relatively easy to incorporate custom defined loss functions or import them from packages like Flux.ml. The interface from Flux.ml is the same as for our loss functions.","category":"page"},{"location":"background/loss_functions/#Poisson-Loss","page":"Loss functions","title":"Poisson Loss","text":"","category":"section"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"As mentioned in Noise Model, Poisson shot noise is usually the dominant source of noise. Therefore one achieves good results by choosing a loss function which considers both the difference between measurement and reconstruction but also the noise process. See Peter J. Verveer , Thomas M. Jovin  (1998) and Jerome Mertz  (2019) for more details on that. As key idea we interpret the measurement as a stochastic process. Our aim is to find a deconvolved image which describes as accurate as possible the measured image. Mathematically the probability for a certain measurement Y is","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"p(Y(r)mu(r)) = prod_r fracmu(r)^Y(r)Gamma(Y(r) + 1) exp(- mu(r))","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"where Y is the measurement, mu is the expected measurement (ideal measurement without noise) and Gamma is the generalized factorial function. In the deconvolution process we get Y as input and want to find the ideal specimen S which results in a measurement mu(r) = (S * textPSF)(r)). Since we want to find the best reconstruction, we want to find a mu(r) so that p(Y(r)  mu(r)) gets as large as possible. Because that means that we find the specimen which describes the measurement with the highest probability. Instead of maximising p(Y(r)  mu(r)) a common trick is to minimise - log(p(Y(r)mu(r))).  Mathematically, the optimization of both functions provides same results but the latter is numerically more stable.","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"$ \\underset{S(r)}{\\arg \\min} (- \\log(p(Y(r)|\\mu(r)))) = \\underset{S(r)}{\\arg \\min} \\sum_r (\\mu(r) + \\log(\\Gamma(Y(r) + 1)) - Y(r) \\log(\\mu(r))$ ","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"which is equivalent to","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"undersetS(r)arg min L = undersetS(r)arg min sum_r (mu(r)  - Y(r) log(mu(r))","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"since the second term only depends on Y(r) but not on mu(r). The gradient of L with respect to mu(r) is simply","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"nabla L = 1 - fracY(r)mu(r)","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"The function L and the gradient nabla L are needed for any gradient descent optimization algorithm. The numerical evaluation of the Poisson loss can lead to issues. Since mu(r)=0 can happen for a measurement with zero intensity background. However, the loss is not defined for mu leq 0. In our source code we set all intensity values below a certain threshold epsilon to epsilon itself. This prevents the evaluation of the logarithm at undefined values.","category":"page"},{"location":"background/loss_functions/#Scaled-Gaussian-Loss","page":"Loss functions","title":"Scaled Gaussian Loss","text":"","category":"section"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"It is well known that the Poisson density function behaves similar as a Gaussian density function for mugg 1. This approximation is almost for all use cases in microscopy valid since regions of interest in an image usually consists of multiple photons and not to a single measured photon. Mathematically the Poisson probability can be approximately (using Stirling's formula in the derivation) expressed as:","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"p(Y(r)mu(r)) approx prod_r fracexp left(-frac(x-mu(r) )^22 mu(r) right)sqrt2 pi  mu(r) ","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"Applying the negative logarithm we get for the loss function:","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"undersetS(r)arg min L = undersetS(r)arg min sum_r frac12 log(mu(r)) + frac(Y(r)-mu(r))^22 mu(r)","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"The gradient is given by:","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"nabla L = fracmu(r) + mu(r)^2 - Y(r)^22 mu^2","category":"page"},{"location":"background/loss_functions/#Gaussian-Loss","page":"Loss functions","title":"Gaussian Loss","text":"","category":"section"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"A very common loss in optimization (and Deep Learning) is a simple Gaussian loss. However, this loss is not recommended for low intensity microscopy since it doesn't considers Poisson noise. However, still combined with suitable regularizer reasonable results can be achieved. The probability is defined as ","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"p(Y(r)mu(r)) = prod_r frac1sqrt2 pi sigma^2 expleft(- frac(Y(r) - mu(r))^22 sigma ^2 right)","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"where sigma is the standard deviation of the Gaussian.","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"Applying the negative logarithm we can simplify the loss to be minimized:","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"undersetS(r)arg min L = undersetS(r)arg min sum_r (Y(r) - mu(r))^2","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"Since we are looking for mu(r) minimizing this expression, sigma is just a constant offset being irrelevant for the solution. This expression is also called L2 loss.","category":"page"},{"location":"workflow/changing_regularizers/#Changing-Regularizers","page":"Changing Regularizers","title":"Changing Regularizers","text":"","category":"section"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers","title":"Changing Regularizers","text":"Load the required modules for these examples:","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers","title":"Changing Regularizers","text":"using DeconvOptim, TestImages, Images, FFTW, Noise, ImageView\n\n# custom image views\nimshow_m(args...) = imshow(cat(args..., dims=3))\nh_view(args...) = begin\n    img = cat(args..., dims=2)\n    img ./= maximum(img)\n    colorview(Gray, img)\nend","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers","title":"Changing Regularizers","text":"As the next step we can prepare a noisy, blurred image. The scaling of img_n is chosen in such a way,  that the highest pixel corresponds to the number of photons measured. The algorithm does not depend critically on that number. However, choosing maximum intensity values of 10 usually introduces some artifacts.","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers","title":"Changing Regularizers","text":"# load test images\nimg = 300 .* convert(Array{Float32}, channelview(testimage(\"resolution_test_512\")))\n\npsf = generate_psf(size(img), 30)\n\n# create a blurred, noisy version of that image\nimg_b = conv_psf(img, psf, [1, 2])\nimg_n = poisson(img_b, 300);\n\nh_view(img, img_b, img_n)","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers","title":"Changing Regularizers","text":"(Image: )","category":"page"},{"location":"workflow/changing_regularizers/#Let's-test-Good's-roughness-(GR)","page":"Changing Regularizers","title":"Let's test Good's roughness (GR)","text":"","category":"section"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers","title":"Changing Regularizers","text":"In this part we can look at the results produced with a GR regularizer. After inspecting the results, it becomes clear, that the benefit of 100 iterations is not really visible. In most cases approx 15 produce good results. By executing GR() we in fact create","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers","title":"Changing Regularizers","text":"@time resGR100, optim_res = deconvolution(img_n, psf, regularizer=GR(), iterations=100)\n@show optim_res\n\n@time resGR15, optim_res = deconvolution(img_n, psf, regularizer=GR(), iterations=15)\n@show optim_res\n\n@time resGR15_2, optim_res = deconvolution(img_n, psf, λ=0.05, regularizer=GR(), iterations=15)\n@show optim_res\n\nh_view(img_n, resGR100, resGR15, resGR15_2)","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers","title":"Changing Regularizers","text":"(Image: )","category":"page"},{"location":"workflow/changing_regularizers/#Let's-test-Total-Variation-(TV)","page":"Changing Regularizers","title":"Let's test Total Variation (TV)","text":"","category":"section"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers","title":"Changing Regularizers","text":"TV produces characteristic staircase artifacts. However, the results it produces are usually noise free and clear.","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers","title":"Changing Regularizers","text":"@time resTV50, optim_res = deconvolution(img_n, psf, regularizer=TV(), iterations=50)\n@show optim_res\n\n@time resTV15, optim_res = deconvolution(img_n, psf, regularizer=TV(), iterations=15)\n@show optim_res\n\n@time resTV15_2, optim_res = deconvolution(img_n, psf, λ=0.005, regularizer=TV(), iterations=15)\n@show optim_res\n\nh_view(img_n, resTV50, resTV15, resTV15_2)","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers","title":"Changing Regularizers","text":"(Image: )","category":"page"},{"location":"workflow/changing_regularizers/#Let's-test-Tikhonov","page":"Changing Regularizers","title":"Let's test Tikhonov","text":"","category":"section"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers","title":"Changing Regularizers","text":"@time resTik1, optim_res = deconvolution(img_n, psf, λ=0.001, regularizer=Tikhonov(), iterations=15)\n@show optim_res\n\n\n@time resTik2, optim_res = deconvolution(img_n, psf, λ=0.0001, \n                    regularizer=Tikhonov(mode=\"spatial_grad_square\"), iterations=15)\n@show optim_res\n\n@time resTik3, optim_res = deconvolution(img_n, psf, λ=0.0001, \n    regularizer=Tikhonov(mode=\"identity\"), iterations=15)\n@show optim_res\n\nh_view(img_n, resTik1, resTik2, resTik3)","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers","title":"Changing Regularizers","text":"(Image: )","category":"page"},{"location":"workflow/changing_regularizers/#Let's-test-without-regularizers","page":"Changing Regularizers","title":"Let's test without regularizers","text":"","category":"section"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers","title":"Changing Regularizers","text":"@jldoctest\n@time res100, optim_res = deconvolution(img_n, psf, regularizer=nothing, iterations=50)\n@show optim_res\n\n@time res15, optim_res = deconvolution(img_n, psf, regularizer=nothing, iterations=15)\n@show optim_res\n\nh_view(img_n, 0.7 .* res100, res15)","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers","title":"Changing Regularizers","text":"(Image: )","category":"page"},{"location":"background/physical_background/#Physical-Background","page":"Physical Background","title":"Physical Background","text":"","category":"section"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"We want to provide some physical background to the process of deconvolution in optics. Optical systems like microscopes can only collect a certain amount of light emitted by a specimen. This effects leads to a blurred image of that specimen. Mathematically the lens has a certain frequency support. Within that frequency transmission of light is supported. Light outside of this frequency support (equivalent to high frequency information) is lost. In the following picture we can see several curves in the frequency domain.  The orange line is a artificial object with a constant frequency spectrum (delta peak). If such a delta peak is transferred through an optical lens, in real space the object is convolved with the point spread function (PSF).  In frequency space such a convolution is a multiplication of the OTF (OTF is the Fourier transform of the PSF) and the frequency spectrum of the object. The green curve is the captured image after transmission through the system. Additionally some noise was introduced which can be recognized through some bumps outside of the OTF support. (Image: Frequency spectrum)","category":"page"},{"location":"background/physical_background/#Forward-Model","page":"Physical Background","title":"Forward Model","text":"","category":"section"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"Mathematically an ideal imaging process of specimen emitting incoherent light by a lens (or any optical system in general) can be described as:","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"Y(r) = (S * textPSF)(r)","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"where * being a convolution, r being the position, S being the sample and textPSF being the point spread function of the system. One can also a background term b independent of the position, which models a constant offset of the imaging sensor:","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"Y(r) = (S * textPSF)(r) + b","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"In frequency space the equation with b=0 is:","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"tilde Y(k) = (tilde S * tildetextPSF)(k)","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"where k is the spatial frequency. From that equation it is clear why the green and blue line in the plot look very similar. The reason is, that the orange line is constant and we basically multiply the OTF with the orange line. ","category":"page"},{"location":"background/physical_background/#Noise-Model","page":"Physical Background","title":"Noise Model","text":"","category":"section"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"However, the physical description (forward model) should also contain a noise term to reflect the measurement process in reality more accurately. ","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"Y(r) = (S * textPSF)(r) + N(r) = mu(r) + N(r)","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"where N being a noise term. In fluorescence microscopy the dominant noise is usually Poisson shot noise (see Jerome Mertz  (2019)). The origin of that noise is the quantum nature of photons. Since the measurement process spans over a time T only a discrete number of photons is detected (in real experiment the amount of photons per pixel is usually in the range of several hundred). Note that this noise is not introduced by the sensor and is just a effect due to quantum nature of light.  We can interprete every sensor pixel as a discrete random variable X. The expected value of that pixel would be mu(r) (true specimen convolved with the textPSF). Due to noise, the systems measures randomly a signal for X according to the Poisson distribution:","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"f(y mu) = fracmu^y exp(-mu)Gamma(y + 1)","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"where f is the probability density distribution, y the measured value of the sensor, mu the expected value and Gamma the generalized factorial function.","category":"page"},{"location":"workflow/changing_loss/#Changing-Loss-Function","page":"Changing Loss Function","title":"Changing Loss Function","text":"","category":"section"},{"location":"workflow/changing_loss/","page":"Changing Loss Function","title":"Changing Loss Function","text":"We can also change the loss function. However, the loss is the most important part guaranteeing good results. Therefore choosing different loss functions than the  provided ones, will most likely to worse results. We try all implemented loss functions of DeconvOptim.jl. However, we could also include loss functions of Flux.jl since they have the same interface.","category":"page"},{"location":"workflow/changing_loss/","page":"Changing Loss Function","title":"Changing Loss Function","text":"Poisson() will most likely produce the best results in presence of Poisson Noise. For Gaussian Noise, Gauss() is a suitable option. ScaledGaussian() is mathematical an approximation of Poisson().","category":"page"},{"location":"workflow/changing_loss/#Code-Example","page":"Changing Loss Function","title":"Code Example","text":"","category":"section"},{"location":"workflow/changing_loss/","page":"Changing Loss Function","title":"Changing Loss Function","text":"using Revise, DeconvOptim, TestImages, Images, FFTW, Noise, ImageView\n\n# custom image views\nimshow_m(args...) = imshow(cat(args..., dims=3))\nh_view(args...) = begin\n    img = cat(args..., dims=2)\n    img ./= maximum(img)\n    colorview(Gray, img)\nend\n\n# load test images\nimg = 300 .* convert(Array{Float32}, channelview(testimage(\"resolution_test_512\")))\n\npsf = generate_psf(size(img), 30)\n\n# create a blurred, noisy version of that image\nimg_b = conv_psf(img, psf, [1, 2])\nimg_n = poisson(img_b, 300);\n\ni@time resP, optim_res = deconvolution(img_n, psf, loss=Poisson(), iterations=10)\n@show optim_res\n\n@time resG, optim_res = deconvolution(img_n, psf, loss=Gauss(), iterations=10)\n@show optim_res\n\n@time resSG, optim_res = deconvolution(img_n, psf, loss=ScaledGauss(), iterations=10)\n@show optim_res\n\nh_view(resP, resG, resSG)","category":"page"},{"location":"workflow/changing_loss/","page":"Changing Loss Function","title":"Changing Loss Function","text":"The left image is Poisson(), in the middle Gauss(). The right image is ScaledGauss(). (Image: )","category":"page"},{"location":"background/mathematical_optimization/#Mathematical-Optimization","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"","category":"section"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"Deconvolution was already described as an optimization problem in the 1970s by L.~B. { Lucy }  (1974), William Hadley Richardson  (1972). Since then, many variants and different kinds of deconvolution algorithms were presented, but mainly based on the concept of Lucy-Richardson. We try to formulate convolution as an inverse physical problem and solve it using a convex optimization loss function so that we can use fast optimizers like L-BFGS to find the optimum. The variables we want to optimize for, are the pixels of the reconstruction S(r). Therefore our reconstruction problem consists of several thousands to billion variables. Mathematical the optimization can be written as:","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"undersetS(r)arg min L(textFwd(S(r))) + textReg(S(r))","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"where textFwd represents the forward model (in our case convolution of S(r) with the textPSF), S(r) is ideal reconstruction, L the loss function and textReg is a regularizer. The regularizer  puts in some prior information about the structure of the object.  See the following sections for more details about each part.","category":"page"},{"location":"background/mathematical_optimization/#Map-Functions","page":"Mathematical Optimization","title":"Map Functions","text":"","category":"section"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"In some cases we want to restrict the optimizer to solutions with S(r) geq 0. Usually one uses boxed optimizer or penalties to prevent negativity. However, in some cases, a S(r)  0 can lead to issues during the optimization process. For that purpose we can introduce a mapping function. Instead of optimizing for S(r) we can optimize for some hat S(r) where M is the mapping function connection ","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"S(r)= M(hat S(r))","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"A simple mapping function leading to S(r) geq 0 is ","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"M(hat S(r)) = hat S(r)^2","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"The optimization problem is then given by","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"undersethat S(r)arg min L(textFwd(M(hat S(r)))) + textReg(M(hat S(r)))","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"After the optimization we need apply M on hat S to get the reconstructed sample ","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"S(r) = M(hat S(r))","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"One could also choose different functions M to obtain reconstruction in certain intensity intervals.","category":"page"},{"location":"background/regularizer/#Regularizer","page":"Regularizer","title":"Regularizer","text":"","category":"section"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"Regularizer are commonly used in inverse problems and especially in deconvolution to obtain solutions which are optimal with respect to some prior.  So far we have included three common regularizer. The regularizer take the current reconstruction S(r) as argument and return a scalar value. This value should be also minimized and is also added to the loss function. Each regularizer produces some characteristic image styles.","category":"page"},{"location":"background/regularizer/#Good's-Roughness-(GR)","page":"Regularizer","title":"Good's Roughness (GR)","text":"","category":"section"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"The Good's roughness definition was taken from I. J. Good , R. A. Gaskins  (1971) and Peter J. Verveer , Thomas M. Jovin  (1998). For Good's roughness several identical expressions can be derived. We implemented the following one:","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"textReg(S(r)) = sum_r sqrtS(r) (Delta_N sqrtS)(r)","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"where N is the dimension of S(r). sqrt S is applied elementwise. Delta_d S(r) is the n-dimensional discrete Laplace operator. As 2D example where r = (xy):","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"(Delta_N sqrtS)(r) = fracsqrtS(x + s_x y) + sqrtS(x - s_x y) + sqrtS(x y+s_y) + S(x y-s_y) - 4 cdot sqrtS(x y)s_x cdot s_y","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"where s_x and s_y are the stencil width in the respective dimension. The Laplace operator can be straightforwardly generalized to n dimensions. ","category":"page"},{"location":"background/regularizer/#Total-Variation-(TV)","page":"Regularizer","title":"Total Variation (TV)","text":"","category":"section"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"As the name suggests, Total variation tries to penalize variation in the image intensity. Therefore it sums up the gradient strength at each point of the image. In 2D this is:","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"textReg(S(r)) = sum_r  (nabla S)(r)","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"Since we look at the magnitude of the gradient strength, this regularizer is anisotropic.","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"In 2D this is:","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"textReg(S(r)) = sum_xy sqrtS(x + 1 y) - S(x y)^2 + S(x y + 1) - S(x y)^2","category":"page"},{"location":"background/regularizer/#Tikhonov-Regularization","page":"Regularizer","title":"Tikhonov Regularization","text":"","category":"section"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"The Tikhonov regularizer is not as specific defined as Good's Roughness or Total Variation. In general Tikhonov regularization is defined by:","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"textReg(S(r)) =  (Gamma S)(r) _2^2","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"where Gamma is an operator which can be chosen freely. Common options are the identity operator which penalizes therefore just high intensity values. Another option would be the spatial gradient which would result in a similar operator to TV. And the last option we implemented is the spatial Laplace.","category":"page"},{"location":"function_references/utils/#Util-functions","page":"Util functions","title":"Util functions","text":"","category":"section"},{"location":"function_references/utils/#Convolution-Functions","page":"Util functions","title":"Convolution Functions","text":"","category":"section"},{"location":"function_references/utils/","page":"Util functions","title":"Util functions","text":"conv_psf\nconv_otf\nconv_otf_r\nplan_conv_r","category":"page"},{"location":"function_references/utils/#DeconvOptim.conv_psf","page":"Util functions","title":"DeconvOptim.conv_psf","text":"conv_psf(obj, psf [, dims])\n\nConvolve obj with psf over dims dimensions. Based on FFT convolution.\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#DeconvOptim.conv_otf","page":"Util functions","title":"DeconvOptim.conv_otf","text":"conv_otf(obj, otf [, dims])\n\nPerforms a FFT-based convolution of an obj with an otf. otf = fft(psf). The 0 frequency of the otf must be located at position [1, 1, 1]. The obj can be of arbitrary dimension but ndims(psf) ≥ ndims(otf). The convolution happens over the dims array. Any further dimensions are broadcasted. Per default dims = [1, 2].\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#DeconvOptim.conv_otf_r","page":"Util functions","title":"DeconvOptim.conv_otf_r","text":"conv_otf_r(obj, otf [, dims])\n\nPerforms a FFT-based convolution of an obj with an otf. Same arguments as conv_otf but with obj being real and otf=rfft(psf).\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#DeconvOptim.plan_conv_r","page":"Util functions","title":"DeconvOptim.plan_conv_r","text":"plan_conv_r(psf [, dims])\n\nPre-plan an optimized convolution for array shaped like psf (based on pre-plan FFT)  along the given dimenions dims. dims = [1, 2] per default. The 0 frequency of psf must be located at [1, 1, 1]. We return first the otf (obtained by rfft(psf)). The second return is the convolution function conv. conv itself has two arguments. conv(obj, otf) where obj is the object and otf the otf.\n\nThis function achieves faster convolution than conv_psf(obj, psf).\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#Point-Spread-Function","page":"Util functions","title":"Point Spread Function","text":"","category":"section"},{"location":"function_references/utils/","page":"Util functions","title":"Util functions","text":"generate_psf","category":"page"},{"location":"function_references/utils/#DeconvOptim.generate_psf","page":"Util functions","title":"DeconvOptim.generate_psf","text":"generate_psf(psf_size, radius)\n\nGeneration of an approximate 2D PSF. psf_size is the output size of the PSF. The PSF will be centered around the point [1, 1], radius indicates the pupil diameter in pixel from which the PSF is generated.\n\nExamples\n\njulia> generate_psf([5, 5], 2)\n5×5 Array{Float64,2}:\n 0.36       0.104721    0.0152786    0.0152786    0.104721\n 0.104721   0.0304627   0.00444444   0.00444444   0.0304627\n 0.0152786  0.00444444  0.000648436  0.000648436  0.00444444\n 0.0152786  0.00444444  0.000648436  0.000648436  0.00444444\n 0.104721   0.0304627   0.00444444   0.00444444   0.0304627\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#Interpolation-and-downsampling","page":"Util functions","title":"Interpolation and downsampling","text":"","category":"section"},{"location":"function_references/utils/","page":"Util functions","title":"Util functions","text":"generate_downsample\nmy_interpolate","category":"page"},{"location":"function_references/utils/#DeconvOptim.generate_downsample","page":"Util functions","title":"DeconvOptim.generate_downsample","text":"generate_downsample(num_dim, downsample_dims, factor)\n\nGenerate a function (based on Tullio.jl) which can be used to downsample arrays. num_dim (Integer) are the dimensions of the array. downsample_dims is a list of which dimensions should be downsampled. factor is a downsampling factor. It needs to be an integer number.\n\nExamples\n\njulia> ds = generate_downsample(2, [1, 2], 2) \n[...]\njulia> ds([1 2; 3 4; 5 6; 7 8])\n2×1 Array{Float64,2}:\n 2.5\n 6.5\n\njulia> ds = generate_downsample(2, [1], 2)\n[...]\njulia> ds([1 2; 3 5; 5 6; 7 8])\n2×2 Array{Float64,2}:\n 2.0  3.5\n 6.0  7.0\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#DeconvOptim.my_interpolate","page":"Util functions","title":"DeconvOptim.my_interpolate","text":"my_interpolate(arr, size_n, [interp_type])\n\nInterpolates arr to the sizes provided in size_n. Therefore it holds ndims(arr) == length(size_n). interp_type specifies the interpolation type. See Interpolations.jl for all options\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#Center-Methods","page":"Util functions","title":"Center Methods","text":"","category":"section"},{"location":"function_references/utils/","page":"Util functions","title":"Util functions","text":"center_extract\ncenter_set!\ncenter_pos","category":"page"},{"location":"function_references/utils/#DeconvOptim.center_extract","page":"Util functions","title":"DeconvOptim.center_extract","text":"center_extract(arr, new_size)\n\nExtracts a center of an array.  new_size must be list of sizes indicating the output size of each dimension. Centered means that a center frequency stays at the center position. Works for even and uneven. If length(new_size) < length(size(arr)) the remaining dimensions are untouched and copied.\n\nExamples\n\njulia> center_extract([[1,2] [3, 4]], [1])\n1×2 Array{Int64,2}:\n 2  4\n\njulia> center_extract([[1,2] [3, 4]], [1, 1])\n1×1 Array{Int64,2}:\n4\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#DeconvOptim.center_set!","page":"Util functions","title":"DeconvOptim.center_set!","text":"center_set!(arr_large, arr_small)\n\nPuts the arr_small central into arr_large. The convention, where the center is, is the same as the definition as for FFT based centered. Function works both for even and uneven arrays.\n\nExamples\n\njulia> center_set!([1, 1, 1, 1, 1, 1], [5, 5, 5])\n6-element Array{Int64,1}:\n 1\n 1\n 5\n 5\n 5\n 1\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#DeconvOptim.center_pos","page":"Util functions","title":"DeconvOptim.center_pos","text":"center_pos(x)\n\nCalculate the position of the center frequency. Size of the array is x\n\nExamples\n\njulia> center_pos(3)\n2\njulia> center_pos(4)\n4\n\n\n\n\n\n","category":"function"},{"location":"workflow/basic_workflow/#Basic-Workflow","page":"Basic Workflow","title":"Basic Workflow","text":"","category":"section"},{"location":"workflow/basic_workflow/","page":"Basic Workflow","title":"Basic Workflow","text":"In this section we show the workflow for deconvolution of 2D and 3D images using different regularizers.  From these examples one can also see the different effects of the regularizers.","category":"page"},{"location":"workflow/basic_workflow/","page":"Basic Workflow","title":"Basic Workflow","text":"The picture below shows the general principle of DeconvOptim.jl. The different parts of the pipeline can be exchanged and adapted to the users needs. In most cases changing the regularizer or the iterations is enough. However, exchanging all different parts is possible. For all options, see the function references. Via the help of Julia (typing ] in the REPL) we can also access extensive help.","category":"page"},{"location":"#DeconvOptim.jl","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"","category":"section"},{"location":"","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"An framework for fast deconvolution of images convolved with a Point Spread Function (PSF).","category":"page"},{"location":"#Overview","page":"DeconvOptim.jl","title":"Overview","text":"","category":"section"},{"location":"","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"In optics, especially in microscopy, measurements are performed with lenses. These lenses support only certain frequencies and weaken the contrast of high frequency content. Furthermore in many cases Poisson or Gaussian noise is introduced by the  quantum nature of light (Poisson shot noise) or sensors (readout noise). DeconvOptim.jl is a Julia solution to that deconvolution process. Our framework relies on several other tools: The deconvolution problem is stated as a convex optimization problem (loss function). Hence we make use of Optim.jl and especially fast solvers like L-BFGS. Since such solvers require gradients (of the loss function) we use the automatic differentiation (AD) of Zygote.jl for that. Of course, one could derive the gradient by hand, however that's error-prone and for some regularizers hard to do by hand. Furthermore, fast AD of the regularizers are hard to achieve if the gradients are written with for loops. Fortunately Tullio.jl provides an extensive and fast framework to get expressions which can derived by the AD in acceptable speed.","category":"page"},{"location":"#Installation","page":"DeconvOptim.jl","title":"Installation","text":"","category":"section"},{"location":"","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"To get the latest stable release of DevonOptim.jl type ] in the Julia REPL:","category":"page"},{"location":"","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"] add DeconvOptim","category":"page"},{"location":"#Quick-Example","page":"DeconvOptim.jl","title":"Quick Example","text":"","category":"section"},{"location":"","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"Below is a quick example how to deconvolve a image which is blurred with a Gaussian Kernel.","category":"page"},{"location":"","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"using Revise # for development useful\nusing DeconvOptim, TestImages, Images, FFTW, Noise\n\n# load test image\nimg = channelview(testimage(\"resolution_test_512\"))\n\n# generate simple Point Spread Function of aperture radius 30\npsf = generate_psf(size(img), 30)\n\n# create a blurred, noisy version of that image\nimg_b = conv_psf(img, psf)\nimg_n = poisson(img_b, 300)\n\n# deconvolve 2D with default options\n@time res, o = deconvolution(img_n, psf)\n\n# show final results next to original and blurred version\ncolorview(Gray, [img img_n res])","category":"page"},{"location":"","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"Left the original image. In the middle the noisy and blurred version. The right images is the deconvolved image with default options. (Image: )","category":"page"}]
}
